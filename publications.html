<!DOCTYPE>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rezwan Matin - Publications</title>
    <link rel="stylesheet" type="text/css" href="publications_style.css" />
  </head>
  <body>
    <div class="container">
      <div class="nav">
	<div id="about_me">
	  <a href="index.html">About Me</a>
	</div>
	<div id="experience">
	  <a href="experience.html">Experience</a>
	</div>
	<div id="skills">
	  <a href="skills.html">Skills</a>
	</div>
	<div id="projects">
	  <a href="projects.html">Projects</a>
	</div>
	<div id="education">
	  <a href="education.html">Education</a>
	</div>
	<div id="publications">
	  <a href="publications.html">Publications</a>
	</div>
	<div id="awards_and_honors">
	  <a href="awards_and_honors.html">Awards and Honors</a>
	</div>
	<div id="test_scores">
	  <a href="test_scores.html">Test Scores</a>
	</div>
	<div id="contact_me">
	  <a href="contact_me.html">Contact Me</a>
	</div>
      </div>  
      <div id="header">
	<h1>Publications</h1>
      </div>
      <div id="toc">
	<h2>Contents</h2>
	<ul>
	  <li class="heading2"><a href="#research">Research</a></li>
	  <ul>
	    <li><a href="#an-audio-processing-approach-using-ensemble-learning-for-speech-emotion-recognition-for-children-with-asd">An Audio Processing Approach using Ensemble Learning for Speech-Emotion Recognition for Children with ASD</a></li>
	    <li><a href="#a-speech-emotion-recognition-solution-based-on-support-vector-machine-for-children-with-autism-spectrum-disorder-to-help-identify-human-emotions">A Speech Emotion Recognition Solution-based on Support Vector Machine for Children with Autism Spectrum Disorder to Help Identify Human Emotions</a></li>
	    <li><a href="#material-and-performance-analysis-of-mems-piezoresistive-pressure-sensor">Material and Performance Analysis of MEMS Piezoresistive Pressure Sensor</a></li>
	  </ul>
        </ul>
      </div>
      <div class="images">
	<div id="ieee" alt="IEEE">
	  <img src="ieee_xplore.jpg" width=180px />
	</div>
	<div id="aiiot" alt="AIIoT">
	  <img src="world_aiiot.png" width=180px />
	</div>
	<div id="ietc" alt="I-ETC">
	  <img src="ietc.png" width=180px />
	</div>
	<div id="ijett" alt="IJETT">
	  <img src="ijett.png" width=180px />
	</div>
      </div>
      <div class="text">
	<section id="research">
	<h2>
	  <a href="#research">Research</a>
	</h2>
	</section>
	  <section id="an-audio-processing-approach-using-ensemble-learning-for-speech-emotion-recognition-for-children-with-asd">
	  <h3>
	    <a href="#an-audio-processing-approach-using-ensemble-learning-for-speech-emotion-recognition-for-children-with-asd">An Audio Processing Approach using Ensemble Learning for Speech-Emotion Recognition for Children with ASD</a>
	  </h3>
	  <p><i>2021 IEEE World AI IoT Congress (AIIoT)</i>, Seattle, WA, USA, May 2021.</p>
	  <p>Children with Autism Spectrum Disorder (ASD) find it difficult to detect human emotions in social interactions. A speech emotion recognition system was developed in this work, which aims to help these children to better identify the emotions of their communication partner. The system was developed using machine learning and deep learning techniques. Through the use of ensemble learning, multiple machine learning algorithms were joined to provide a final prediction on the recorded input utterances. The ensemble of models includes a Support Vector Machine (SVM), a Multi-Layer Perceptron (MLP), and a Recurrent Neural Network (RNN). All three models were trained on the Ryerson Audio-Visual Database of Emotional Speech and Songs (RAVDESS), the Toronto Emotional Speech Set (TESS), and the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D). A fourth dataset was used, which was created by adding background noise to the clean speech files from the datasets previously mentioned. The paper describes the audio processing of the samples, the techniques used to include the background noise, and the feature extraction coefficients considered for the development and training of the models. This study presents the performance evaluation of the individual models to each of the datasets, inclusion of the background noises, and the combination of using all of the samples in all three datasets. The evaluation was made to select optimal hyperparameters configuration of the models to evaluate the performance of the ensemble learning approach through majority voting. The overall performance of the ensemble learning reached a peak accuracy of 66.5%, reaching a higher performance emotion classification accuracy than the MLP model which reached 65.7%. <br> (To read the full article, click <a href="https://ieeexplore.ieee.org/abstract/document/9454174" target="_blank">here</a>.)</p><br>
	  </section>
	  <section id="a-speech-emotion-recognition-solution-based-on-support-vector-machine-for-children-with-autism-spectrum-disorder-to-help-identify-human-emotions">
	  <h3>
	    <a href="#a-speech-emotion-recognition-solution-based-on-support-vector-machine-for-children-with-autism-spectrum-disorder-to-help-identify-human-emotions">A Speech Emotion Recognition Solution-based on Support Vector Machine for Children with Autism Spectrum Disorder to Help Identify Human Emotions</a>
	  </h3>
	  <p><i>IEEE Intermountain Engineering, Technology, and Computing Conference (I-ETC)</i>, Orem, UT, USA, October 2020.</p>
	  <p>Children who fall into the autism spectrum have difficulty communicating with others. In this work, a speech emotion recognition model has been developed to help children with Autism Spectrum Disorder (ASD) identify emotions in social interactions. The model is created using the Python programming language to develop a machine learning model based on the Support Vector Machine (SVM). SVM has proven to yield high accuracies when classifying inputs in speech processing. Individual audio databases are specifically designed to train models for the emotion recognition task. One such speech corpus is the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), which is used to train the model in this work. Acoustic feature extraction will be part of the pre-processing step utilizing Python libraries. The libROSA library is used in this work. The first 26 Mel-frequency Cepstral Coefficients (MFCCs) and the zero-crossing rate (ZCR) are extracted and used as the acoustic features to train the machine learning model. The final SVM model provided a test accuracy of 77%. This model also performed well when significant background noise was introduced to the RAVDESS audio recordings, for which it yielded a test accuracy of 64%. <br> (To read the full article, click <a href="https://ieeexplore.ieee.org/abstract/document/9249147" target="_blank">here</a>.)</p><br>
	  </section>
	  <section id="material-and-performance-analysis-of-mems-piezoresistive-pressure-sensor">
	  <h3>
	    <a href="#material-and-performance-analysis-of-mems-piezoresistive-pressure-sensor">Material and Performance Analysis of MEMS Piezoresistive Pressure Sensor</a>
	  </h3>
	  <p><i>International Journal of Engineering Trends and Technology (IJETT)</i>, Volume-31, Number-1, January 2016 issue.</p>
	  <p>This work focuses on MEMS piezoresistive pressure sensor. The sensor was simulated in COMSOL Multiphysics v4.4. The Motorola MPX100 series sensor was studied. Applied pressure range is varied from 0 to 100 kPa. To gain the optimum output, different combination of material for diaphragm & piezoresistor have been studied and corresponding displacement change, shear stress distribution and output voltage have been shown. Sensitivity of the sensor was also calculated for different combination of materials. Impact of doping concentration on output voltage for both diaphragm & piezoresistor material has also been studied. <br> (To read the full article, click <a href="http://www.ijettjournal.org/archive/ijett-v31p202" target="_blank">here</a>.)</p>
	  </section>
        <a id="back-to-top" href="#">Back to top</a>
      </div>
      <div id="footer">
	Copyright &copy; 2025 Rezwan Matin.
      </div>
    </div>
  </body>
</html>
